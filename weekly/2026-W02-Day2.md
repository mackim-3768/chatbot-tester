# Daily Framework Planning – 2026-W02-Day2

> Repository: chatbot-tester
> Type: Library / SDK (Generator · Runner · Evaluator)
> Cadence: Daily (주차 내 일차별 사고 기록)
> Focus: Architecture, API design, extensibility, reproducibility

---

## Language Rule (MANDATORY)

- 본 문서의 **모든 내용은 반드시 한국어로 작성**합니다.
- 다음 항목만 예외적으로 영어 사용을 허용합니다:
  - 코드 블록
  - 파일 경로
  - 함수명 / 클래스명 / 모듈명
  - CLI 커맨드
  - 고유 명사 (예: OpenAI, CLI, SDK, Generator, Runner, Evaluator)
- 번역체가 아닌 **기술 기획 문서에 적합한 자연스러운 한국어**를 사용합니다.

---

## 1. Discovery (Ideas)

> Goal: 재사용 가능한 SDK 관점에서의 프레임워크 수준 개선 아이디어를
> **하루 단위로 축적**한다.

### Idea 1
- Affected layer: Common / Configuration
- Current limitation: Generator, Runner, Evaluator의 설정이 파편화되어 있어(PipelineOptions, RunnerOptions, EvaluatorConfig) 하나의 완전한 실험(Experiment)을 재현하기 어렵습니다.
- Proposed improvement: `ExperimentProfile`이라는 통합 구성 스키마를 도입하여 데이터 생성부터 평가까지의 전체 라이프사이클을 단일 YAML로 정의하도록 합니다.
- Why this matters for an SDK: 사용자가 실험 설정을 단일 파일로 버전 관리하고 공유할 수 있어 재현성(Reproducibility)이 향상됩니다.

### Idea 2
- Affected layer: Runner
- Current limitation: `run_async_job` 실행 시 모든 `RunResult`를 메모리에 보관하다가 마지막에 반환합니다. 대규모 데이터셋 실행 시 OOM(Out of Memory) 위험이 있고, 중간에 실패하면 데이터가 유실됩니다.
- Proposed improvement: 실행 결과를 실시간으로 저장하거나 스트리밍 처리할 수 있는 `ResultHandler` 또는 `ArtifactStore` 추상화를 도입합니다.
- Why this matters for an SDK: 대규모 벤치마크 수행 시 안정성과 확장성(Scalability)을 보장하기 위해 필수적입니다.

### Idea 3
- Affected layer: Generator
- Current limitation: 데이터 전처리 로직(`canonicalize`, `filter`, `sample`)이 `run_pipeline` 함수 내에 하드코딩되어 있어 확장이 어렵습니다.
- Proposed improvement: 변환 단계를 모듈화하여 사용자가 임의의 Transformer 체인을 구성할 수 있는 Pipeline 패턴을 적용합니다.
- Why this matters for an SDK: 사용자가 데이터셋 특성에 맞는 커스텀 전처리 로직을 쉽게 주입할 수 있어야 합니다.

### Idea 4
- Affected layer: Evaluator
- Current limitation: 평가 결과 분석 시 `tag`, `language`, `length` 등 미리 정의된 차원으로만 분류(Breakdown)가 가능합니다.
- Proposed improvement: 메타데이터의 임의 필드를 기준으로 분류할 수 있도록 동적 Dimension 추출 기능을 제공합니다.
- Why this matters for an SDK: 고정된 차원 외에 사용자 정의 메타데이터를 기반으로 한 유연한 분석이 필요합니다.

### Idea 5
- Affected layer: CLI
- Current limitation: 각 모듈(Generator, Runner, Evaluator)별로 CLI 진입점이 분리되어 있어 통합된 사용자 경험을 제공하지 못합니다.
- Proposed improvement: `ct` (chatbot-tester)와 같은 단일 통합 CLI 명령어를 제공하고, 서브커맨드로 각 단계를 제어하도록 합니다.
- Why this matters for an SDK: 도구의 사용성을 높이고 워크플로우를 표준화할 수 있습니다.

---

## 2. Triage

> Goal: **오늘 기준** 가장 아키텍처적 파급력이 큰 1개 아이디어를 선정한다.

### Selected Idea
- Title: Unified Experiment Profile (통합 실험 구성 전략)
- Primary affected layer(s): Common, Generator, Runner, Evaluator

### Selection Rationale
- Architectural leverage: 프레임워크의 모든 컴포넌트를 아우르는 최상위 설정 구조를 정의함으로써, 파편화된 설정을 통합하고 일관성을 부여합니다.
- Impact on extensibility / reuse: 실험 정의가 코드와 분리되어 데이터 기반의 실험 관리가 가능해집니다.
- Reduction of future complexity: 각 단계별 설정을 개별적으로 관리함에 따른 복잡도를 낮추고, 향후 파이프라인 자동화의 기반이 됩니다.

### Deferred Ideas (Brief)
- Idea 2 (Result Persistence): 중요하지만, 통합 설정 구조가 먼저 잡히면 그 안에서 스토리지 설정을 다룰 수 있으므로 순위가 밀립니다.
- Idea 3 (Generator Pipeline): Generator 내부 구현 상세에 가까우며, 전체 아키텍처 구조보다는 모듈 내부 개선 사항입니다.

---

## 3. Spec Draft (Top 1 Only)

### Feature / Improvement Name
Unified Experiment Profile (`ExperimentProfile`)

### Problem Statement
- 현재 Generator(`PipelineOptions`), Runner(`RunnerOptions`), Evaluator(`EvaluatorConfig`)의 설정 방식과 포맷이 제각각입니다.
- 이로 인해 "특정 데이터셋으로, 특정 모델을 사용하여, 특정 지표로 평가한다"는 하나의 완결된 실험 시나리오를 정의하고 공유하기 어렵습니다.
- 라이브러리 사용자는 각 단계를 수동으로 연결해야 하며, 설정 불일치로 인한 오류가 발생하기 쉽습니다.

### Design Approach (High-level)
- Core concept: `experiment.yaml` 파일 하나로 전체 워크플로우를 제어합니다.
- Key abstractions or interfaces:
  - `ExperimentConfig` (Root Pydantic Model)
  - `DatasetConfig` (Generator 옵션 포함)
  - `ExecutionConfig` (Runner 옵션 포함)
  - `EvaluationConfig` (기존 `EvaluatorConfig` 활용 및 확장)
- Expected behavior:
  - 사용자는 YAML 파일을 로드하여 `Experiment` 객체를 생성합니다.
  - 통합 CLI 또는 스크립트에서 이 객체를 통해 전 과정을 순차적 또는 선택적으로 실행할 수 있습니다.

### MVP Scope
- Included in MVP:
  - `chatbot_tester.common.config` 모듈 신설
  - Pydantic 기반의 계층적 설정 스키마 정의 (`ExperimentConfig`, `DatasetConfig`, `ExecutionConfig`)
  - YAML 파일 로더 및 유효성 검사 로직 구현
- Explicitly excluded:
  - 통합 CLI (`ct`) 구현 (별도 과제로 진행)
  - 실험 결과를 DB에 저장하거나 시각화하는 대시보드 연동

### Optional / Future Extensions
- 실험 상속(Inheritance): 기본 설정을 정의해두고 일부만 오버라이딩하여 변형 실험 생성
- 환경 변수 치환: YAML 내 `${ENV_VAR}` 문법 지원

### Acceptance Criteria
- [ ] `ExperimentConfig` Pydantic 모델이 정의되고, YAML 파일로부터 로드 및 검증이 가능해야 함
- [ ] `DatasetConfig`가 기존 `PipelineOptions`의 필수 항목을 커버해야 함
- [ ] `ExecutionConfig`가 기존 `RunnerOptions`와 `RunConfig`를 포함해야 함
- [ ] 잘못된 설정 파일 입력 시 명확한 유효성 검사 에러를 반환해야 함

---

## 4. Backlog Draft (Issue-Level)

### Suggested GitHub Issue Title
`[Framework][Daily][2026-W02-Day2] Unified Experiment Profile 스키마 및 로더 구현`

### Task Breakdown
- [ ] Core implementation
  - Module: `chatbot_tester.common.config`
  - Summary: `experiment.yaml`을 처리하기 위한 Pydantic 모델 계층 구조(`ExperimentConfig`, `DatasetConfig`, `ExecutionConfig`) 정의 및 로더 구현
- [ ] CLI changes (if any)
  - Command: N/A (이번 작업은 스키마 정의에 집중)
  - Flags / options: N/A
- [ ] API impact
  - Breaking change: No (기존 클래스들은 유지하되, 내부적으로 변환 로직 추가 예정)
  - Migration needed: No (신규 기능)
- [ ] Tests
  - Unit / Integration: 유효한/유효하지 않은 YAML 설정 파일에 대한 파싱 및 검증 테스트 케이스 작성
- [ ] Documentation
  - README / Examples / Docstrings: `experiment.yaml` 예제 파일 및 스키마 설명 추가

### Notes
- Backward compatibility concerns: 기존 `EvaluatorConfig`는 유지하며, `EvaluationConfig`가 이를 포함하거나 호환되도록 설계해야 합니다.
- Refactoring risk: 설정 클래스 이름 충돌에 유의해야 합니다.

---

## 5. Docs / Notes

### README Updates
- 추가 또는 수정할 섹션: "Configuration" 또는 "Defining Experiments" 섹션 추가
- 전달하고자 하는 핵심 메시지: 복잡한 실험 설정을 단일 YAML 파일로 관리하여 재현성을 확보하는 방법 안내

### Example Snippet (Optional)

```yaml
# experiment.yaml example
version: "1.0"
metadata:
  name: "gpt4-korean-summary-test"
  description: "GPT-4 한국어 요약 성능 벤치마크"
  tags: ["korean", "summary", "v1"]

dataset:
  source: "./data/raw_logs.csv"
  output_dir: "./data/processed"
  filter:
    min_len: 10
    max_len: 1000

execution:
  backend: "openai-chat"
  model: "gpt-4"
  concurrency: 5
  timeout: 60

evaluation:
  metrics:
    - type: "bleu"
    - type: "llm_judge"
      name: "coherence"
      parameters:
        model: "gpt-4"
```
