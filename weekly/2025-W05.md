# Weekly Framework Planning – 2025-W05

> Repository: chatbot-tester
> Type: Library / SDK (Generator · Runner · Evaluator)
> Focus: Architecture, API design, extensibility, reproducibility

---

## 1. Discovery (Ideas)

> Goal: Identify framework-level improvements suitable for a reusable SDK.

### Idea 1: Unified Pipeline Configuration & Orchestrator
- **Affected layer**: Common / All
- **Current limitation**: Running a full test cycle requires three separate CLI commands (`gen-dataset`, `runner`, `evaluator`) and manual management of intermediate file paths (e.g., passing the output of Runner to Evaluator).
- **Proposed improvement**: Introduce a `PipelineConfig` (YAML) and a top-level `orchestrator` module that executes the full sequence (Generator -> Runner -> Evaluator) automatically, handling artifact passing.
- **Why this matters for an SDK**: It defines a standard "experiment" unit, ensuring reproducibility and simplifying integration into CI/CD pipelines (one command vs script hacking).

### Idea 2: Plugin System via Entry Points
- **Affected layer**: Runner / Evaluator
- **Current limitation**: Adding a new Backend or Metric currently requires importing the code and manually registering it with decorators, or modifying the library source.
- **Proposed improvement**: Use Python `importlib.metadata` entry points (e.g., `chatbot_tester.backends`) to automatically discover and register plugins installed in the environment.
- **Why this matters for an SDK**: Allows third-party developers to create valid extensions (e.g., `chatbot-tester-bedrock`) without forking or modifying the core library.

### Idea 3: Workspace / Artifact Manager
- **Affected layer**: Common
- **Current limitation**: Directory structures (`dataset/`, `runs/`) are created via shell scripts (`mkdir -p`), leading to "magic string" paths and potential overwrites.
- **Proposed improvement**: Implement a `Workspace` class that enforces a strict directory layout, handles file validation, and manages run IDs/timestamps.
- **Why this matters for an SDK**: Improves reliability and developer experience by abstracting file system operations and ensuring consistent output structure.

### Idea 4: Standardized Token Usage & Cost Tracking
- **Affected layer**: Runner
- **Current limitation**: `RunResult` tracks latency but lacks standardized fields for token usage (prompt/completion) and estimated cost, which varies by backend.
- **Proposed improvement**: Add `usage` and `cost` fields to `RunResult` and update Backends to populate them.
- **Why this matters for an SDK**: Critical for operational monitoring and budget management when running large-scale evaluations.

### Idea 5: Differential / Regression Evaluation Reports
- **Affected layer**: Evaluator
- **Current limitation**: Reports currently analyze a single run. Users cannot easily compare "Model V1" vs "Model V2" side-by-side to see regression.
- **Proposed improvement**: Add a "Diff" or "Compare" mode to the Evaluator that takes multiple run results and generates a comparative report (e.g., delta in scores).
- **Why this matters for an SDK**: Essential for the "testing" aspect of the framework (checking if a change caused a regression).

---

## 2. Triage

> Goal: Select the most impactful idea from an architectural perspective.

### Selected Idea
- **Title**: Unified Pipeline Orchestrator
- **Primary affected layer(s)**: Common / Orchestration

### Selection Rationale
- **Architectural leverage**: It bridges the three isolated components (Generator, Runner, Evaluator) into a cohesive framework.
- **Impact on extensibility / reuse**: It establishes a declarative contract (`pipeline.yaml`) for experiments, which is easier to share and version control than shell scripts.
- **Reduction of future complexity**: By centralizing the execution logic, we remove the burden of path management from the user.

### Deferred Ideas (Brief)
- **Plugin System**: Valuable, but the current registry pattern works for now. Can be added after the pipeline structure is solidified.
- **Workspace Manager**: Can be partially implemented as an internal detail of the Pipeline Orchestrator first.
- **Differential Evaluation**: Requires a stable pipeline and artifact structure first.

---

## 3. Spec Draft (Top 1 Only)

### Feature / Improvement Name
**Unified Pipeline Orchestrator**

### Problem Statement
- **Difficult**: Users must manually chain 3+ CLI tools, copy-paste paths, and ensure arguments match.
- **Error-prone**: Easy to evaluate the wrong run file or mismatch metadata.
- **Affected**: All users (CLI and Library).

### Design Approach (High-level)
- **Core concept**: A `Pipeline` object that takes a configuration, creates a workspace, and calls the components in order.
- **Key abstractions**:
  - `PipelineConfig`: A Pydantic model representing the YAML config.
  - `PipelineOrchestrator`: The execution engine.
- **Expected behavior**:
  ```bash
  chatbot-tester run pipeline.yaml
  ```
  This command will generate data (if configured), run the model, and evaluate results, producing a final report in a determined location.

### MVP Scope
- **Included in MVP**:
  - `PipelineConfig` schema definition (Generator params, Runner config, Evaluator config).
  - `orchestrator.run_pipeline()` Python API.
  - CLI command `run`.
  - Automatic directory management for the pipeline run (e.g., `outputs/<run_id>/`).
- **Explicitly excluded**:
  - Parallel execution of multiple pipelines.
  - Complex dependency graphs (DAGs) - strictly linear for now.

### Optional / Future Extensions
- Caching (skip generation if dataset exists).
- Remote execution.

### Acceptance Criteria
- [ ] Users can define a single `pipeline.yaml` that replaces `run_quickstart.sh`.
- [ ] The `run` command executes Generator -> Runner -> Evaluator sequence.
- [ ] Intermediate files are correctly passed between steps without user intervention.
- [ ] A final report is generated.

---

## 4. Backlog Draft (Issue-Level)

### Suggested GitHub Issue Title
`[Framework] Implement Unified Pipeline Orchestrator and Configuration`

### Task Breakdown
- [ ] **Core implementation**
  - Module: `src/chatbot_tester/orchestrator/` (new) and `src/chatbot_tester/common/config.py`
  - Summary: Define `PipelineConfig` Pydantic models. Implement `run_pipeline` function that calls `generator.main`, `runner.run_job`, `evaluator.evaluate`.
- [ ] **CLI changes**
  - Command: `python -m chatbot_tester.cli run <config_path>`
  - Flags: `--dry-run`, `--workspace-dir`.
- [ ] **API impact**
  - Breaking change: No (existing separate CLIs remain).
  - Migration needed: No.
- [ ] **Tests**
  - Integration: Create a test that runs a `pipeline.yaml` using the mock/toy data.
- [ ] **Documentation**
  - README: Add "Pipelines" section.
  - Example: Add `example/pipeline/pipeline.yaml`.

### Notes
- **Refactoring risk**: Low, as we are wrapping existing functionality, not changing internal logic heavily.
- **Backwards Compatibility**: Existing `generator/cli.py`, `runner/cli.py` etc. must remain functional.

---

## 5. Docs / Notes

### README Updates
- **New Section**: "Running Pipelines" - explain how to use the declarative configuration.
- **Key Message**: "For reproducible experiments, use the Pipeline system instead of manual scripts."

### Example Snippet

```yaml
# pipeline.yaml
name: "v1_release_eval"
workspace: "./workspace"

generator:
  source: "./data/raw.csv"
  dataset_id: "support_v1"

runner:
  backend: "openai"
  model: "gpt-4"
  params:
    temperature: 0.7

evaluator:
  metrics:
    - type: "exact_match"
  report:
    formats: ["html", "json"]
```

```python
from chatbot_tester import run_pipeline

# Run the pipeline programmatically
run_pipeline("pipeline.yaml")
```
