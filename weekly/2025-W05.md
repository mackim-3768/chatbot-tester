# 주간 프레임워크 계획 – 2025-W05

> 리포지토리: lm-eval-so
> 유형: 라이브러리 / SDK (Generator · Runner · Evaluator)
> 주안점: 아키텍처, API 디자인, 확장성, 재현성

---

## 1. Discovery (아이디어 발굴)

> 목표: 재사용 가능한 SDK에 적합한 프레임워크 수준의 개선 사항 식별

### 아이디어 1: 통합 파이프라인 구성 및 오케스트레이터 (Unified Pipeline Orchestrator)
- **영향 받는 레이어**: 공통 (Common) / 전체
- **현재 한계**: 전체 테스트 사이클을 실행하려면 3개의 개별 CLI 명령어(`gen-dataset`, `runner`, `evaluator`)를 실행해야 하며, 중간 파일 경로(예: Runner 출력을 Evaluator에 전달)를 수동으로 관리해야 함.
- **제안하는 개선**: `PipelineConfig` (YAML)와 최상위 `orchestrator` 모듈을 도입하여 전체 시퀀스(Generator -> Runner -> Evaluator)를 자동으로 실행하고 아티팩트 전달을 처리함.
- **SDK로서 중요한 이유**: 표준화된 "실험(experiment)" 단위를 정의하여 재현성을 보장하고 CI/CD 파이프라인 통합을 단순화함 (스크립트 작성 대신 단일 명령어로 실행).

### 아이디어 2: 진입점(Entry Points)을 통한 플러그인 시스템
- **영향 받는 레이어**: Runner / Evaluator
- **현재 한계**: 새로운 Backend나 Metric을 추가하려면 코드를 임포트하고 데코레이터로 수동 등록하거나 라이브러리 소스를 수정해야 함.
- **제안하는 개선**: Python `importlib.metadata` 진입점(예: `lm_eval_so.backends`)을 사용하여 환경에 설치된 플러그인을 자동으로 검색하고 등록함.
- **SDK로서 중요한 이유**: 서드파티 개발자가 코어 라이브러리를 포크하거나 수정하지 않고도 유효한 확장(예: `lm-eval-so-bedrock`)을 만들 수 있음.

### 아이디어 3: 워크스페이스 / 아티팩트 매니저
- **영향 받는 레이어**: 공통 (Common)
- **현재 한계**: 디렉터리 구조(`dataset/`, `runs/`)가 쉘 스크립트(`mkdir -p`)를 통해 생성되어 "매직 스트링" 경로와 덮어쓰기 위험이 존재함.
- **제안하는 개선**: 엄격한 디렉터리 레이아웃을 강제하고, 파일 유효성을 검사하며, 실행 ID/타임스탬프를 관리하는 `Workspace` 클래스를 구현함.
- **SDK로서 중요한 이유**: 파일 시스템 작업을 추상화하고 일관된 출력 구조를 보장하여 신뢰성과 개발자 경험을 향상함.

### 아이디어 4: 표준화된 토큰 사용량 및 비용 추적
- **영향 받는 레이어**: Runner
- **현재 한계**: `RunResult`는 지연 시간(latency)은 추적하지만, 백엔드마다 다른 토큰 사용량(프롬프트/완성) 및 추정 비용에 대한 표준화된 필드가 부족함.
- **제안하는 개선**: `RunResult`에 `usage` 및 `cost` 필드를 추가하고 백엔드가 이를 채우도록 업데이트함.
- **SDK로서 중요한 이유**: 대규모 평가 실행 시 운영 모니터링 및 예산 관리에 필수적임.

### 아이디어 5: 차분(Differential) / 회귀(Regression) 평가 리포트
- **영향 받는 레이어**: Evaluator
- **현재 한계**: 리포트는 현재 단일 실행만 분석함. 사용자가 "모델 V1" 대 "모델 V2"를 나란히 비교하여 회귀를 쉽게 확인할 수 없음.
- **제안하는 개선**: Evaluator에 "Diff" 또는 "Compare" 모드를 추가하여 여러 실행 결과를 받아 비교 리포트(예: 점수 델타)를 생성함.
- **SDK로서 중요한 이유**: 프레임워크의 "테스트" 측면(변경으로 인한 회귀 확인)에 필수적임.

---

## 2. Triage (우선순위 결정)

> 목표: 아키텍처 관점에서 가장 영향력 있는 아이디어 선정

### 선정된 아이디어
- **제목**: 통합 파이프라인 오케스트레이터 (Unified Pipeline Orchestrator)
- **주요 영향 레이어**: 공통 / 오케스트레이션

### 선정 근거
- **아키텍처적 레버리지**: 분리된 세 가지 컴포넌트(Generator, Runner, Evaluator)를 하나의 응집력 있는 프레임워크로 연결함.
- **확장성/재사용성 영향**: 쉘 스크립트보다 공유 및 버전 관리가 쉬운 실험용 선언적 계약(`pipeline.yaml`)을 수립함.
- **미래 복잡성 감소**: 실행 로직을 중앙화함으로써 사용자의 경로 관리 부담을 제거함.

### 보류된 아이디어 (요약)
- **플러그인 시스템**: 가치가 있지만 현재 레지스트리 패턴으로도 당장은 작동함. 파이프라인 구조가 견고해진 후 추가 가능.
- **워크스페이스 매니저**: 파이프라인 오케스트레이터의 내부 세부 사항으로 부분 구현 가능.
- **차분 평가**: 안정적인 파이프라인과 아티팩트 구조가 선행되어야 함.

---

## 3. Spec Draft (Top 1 Only)

### 기능 / 개선명
**통합 파이프라인 오케스트레이터 (Unified Pipeline Orchestrator)**

### 문제 정의 (Problem Statement)
- **어려움**: 사용자가 3개 이상의 CLI 도구를 수동으로 연결하고, 경로를 복사-붙여넣기 하며, 인수가 일치하는지 확인해야 함.
- **오류 발생 쉬움**: 잘못된 실행 파일을 평가하거나 메타데이터가 불일치하기 쉬움.
- **영향**: 모든 사용자 (CLI 및 라이브러리).

### 설계 접근 방식 (High-level)
- **핵심 개념**: 구성을 받아 워크스페이스를 생성하고 컴포넌트를 순서대로 호출하는 `Pipeline` 객체.
- **주요 추상화**:
  - `PipelineConfig`: YAML 구성을 나타내는 Pydantic 모델.
  - `PipelineOrchestrator`: 실행 엔진.
- **예상 동작**:
  ```bash
  lm-eval-so run pipeline.yaml
  ```
  이 명령어는 데이터 생성(구성된 경우) -> 모델 실행 -> 결과 평가를 수행하고 정해진 위치에 최종 리포트를 생성함.

### MVP 범위
- **MVP 포함**:
  - `PipelineConfig` 스키마 정의 (Generator 파라미터, Runner 설정, Evaluator 설정).
  - `orchestrator.run_pipeline()` Python API.
  - CLI 명령어 `run`.
  - 파이프라인 실행을 위한 자동 디렉터리 관리 (예: `outputs/<run_id>/`).
- **명시적 제외**:
  - 다중 파이프라인 병렬 실행.
  - 복잡한 의존성 그래프(DAGs) - 현재는 엄격한 선형 구조만 지원.

### 옵션 / 향후 확장
- 캐싱 (데이터셋이 존재하면 생성 건너뛰기).
- 원격 실행.

### 완료 조건 (Acceptance Criteria)
- [ ] 사용자가 `run_quickstart.sh`를 대체하는 단일 `pipeline.yaml`을 정의할 수 있음.
- [ ] `run` 명령어가 Generator -> Runner -> Evaluator 시퀀스를 실행함.
- [ ] 중간 파일이 사용자 개입 없이 단계 간에 올바르게 전달됨.
- [ ] 최종 리포트가 생성됨.

---

## 4. Backlog Draft (Issue-Level)

### 제안된 GitHub Issue 제목
`[Framework] Implement Unified Pipeline Orchestrator and Configuration`

### 작업 세분화 (Task Breakdown)
- [ ] **핵심 구현**
  - 모듈: `src/lm_eval_so/orchestrator/` (신규) 및 `src/lm_eval_so/common/config.py`
  - 요약: `PipelineConfig` Pydantic 모델 정의. `generator.main`, `runner.run_job`, `evaluator.evaluate`를 호출하는 `run_pipeline` 함수 구현.
- [ ] **CLI 변경**
  - 명령어: `python -m lm_eval_so.cli run <config_path>`
  - 플래그: `--dry-run`, `--workspace-dir`.
- [ ] **API 영향**
  - 파괴적 변경: 없음 (기존 개별 CLI는 유지됨).
  - 마이그레이션 필요: 없음.
- [ ] **테스트**
  - 통합 테스트: 모의/토이 데이터를 사용하여 `pipeline.yaml`을 실행하는 테스트 생성.
- [ ] **문서화**
  - README: "Pipelines" 섹션 추가.
  - 예제: `example/pipeline/pipeline.yaml` 추가.

### 참고 사항
- **리팩토링 위험**: 낮음. 내부 로직을 크게 변경하는 것이 아니라 기존 기능을 래핑하는 것이므로.
- **하위 호환성**: 기존 `generator/cli.py`, `runner/cli.py` 등은 계속 작동해야 함.

---

## 5. Docs / Notes

### README 업데이트
- **새 섹션**: "Running Pipelines" - 선언적 구성을 사용하는 방법 설명.
- **핵심 메시지**: "재현 가능한 실험을 위해 수동 스크립트 대신 파이프라인 시스템을 사용하세요."

### 예제 스니펫

```yaml
# pipeline.yaml
name: "v1_release_eval"
workspace: "./workspace"

generator:
  source: "./data/raw.csv"
  dataset_id: "support_v1"

runner:
  backend: "openai"
  model: "gpt-4"
  params:
    temperature: 0.7

evaluator:
  metrics:
    - type: "exact_match"
  report:
    formats: ["html", "json"]
```

```python
from lm_eval_so import run_pipeline

# 프로그래밍 방식으로 파이프라인 실행
run_pipeline("pipeline.yaml")
```
